{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/priya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "file1 = open(\"updated-knock.txt\") \n",
    "content = (file1.read()).lower()# Use this to read file content as a stream: \n",
    "lines = content.split('\\n') \n",
    "item_set=set()\n",
    "updated_sentances=[]\n",
    "# appendFile = open('filteredtext.txt','w') \n",
    "for line in lines:\n",
    "    line = re.sub('[^A-Za-z0-9 ]+', '', line)\n",
    "    words = line.split()\n",
    "    line = []\n",
    "    for r in words:\n",
    "        if r not in stop_words:\n",
    "            item_set.add(r)\n",
    "            line.append(r)\n",
    "    line = \" \".join(line)\n",
    "#     print(line)\n",
    "#     item_set.add(\"'\"+line+\"'\")\n",
    "    updated_sentances.append(line)\n",
    "#     appendFile.write(line+\"\\n\") \n",
    "# appendFile.close() \n",
    "ids = (list(range(1,len(updated_sentances)+1)))\n",
    "for i in range(0,len(ids)):\n",
    "    ids[i] = str(ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "# print(item_set)\n",
    "print(len(item_set))\n",
    "item_set = list(item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputFile = open(\"weka_input.arff\",\"w\")\n",
    "# inputFile.write(\"@relation knockjokes \\n\\n\")\n",
    "# # inputFile.write(\"@attribute id_no {\"+\", \".join(ids)+\"}\\n\")\n",
    "# # inputFile.write(\"@attribute joke {\"+\", \".join(item_set)+\"}\\n\\n\")\n",
    "\n",
    "# for i in item_set:\n",
    "#     inputFile.write(\"@attribute \"+i+\" {'\"+i+\"'}\\n\")\n",
    "# inputFile.write(\"\\n@data\\n\")\n",
    "# # id = 1\n",
    "# for l in updated_sentances:\n",
    "#     wl = l.split()\n",
    "#     row=[]\n",
    "#     for item in item_set:\n",
    "#         if item in wl:\n",
    "#             row.append(item)\n",
    "#         else:\n",
    "#             row.append(\"?\")\n",
    "#     inputFile.write(\",\".join(row)+\"'\\n\")\n",
    "# #     id+=1\n",
    "# inputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile2 = open(\"new_weka_input.arff\",\"w\")\n",
    "inputFile2.write(\"@relation knockjokes \\n\\n\")\n",
    "# inputFile.write(\"@attribute id_no {\"+\", \".join(ids)+\"}\\n\")\n",
    "# inputFile.write(\"@attribute joke {\"+\", \".join(item_set)+\"}\\n\\n\")\n",
    "\n",
    "for i in item_set:\n",
    "    if i not in ['knock','whos']:\n",
    "        inputFile2.write(\"@attribute \"+i+\" {'\"+i+\"'}\\n\")\n",
    "inputFile2.write(\"\\n@data\\n\")\n",
    "# id = 1\n",
    "for l in updated_sentances:\n",
    "    wl = l.split()\n",
    "    row=[]\n",
    "    for i in item_set:\n",
    "        if i not in ['knock','whos']:\n",
    "            if i in wl:\n",
    "                row.append(i)\n",
    "            else:\n",
    "                row.append(\"?\")\n",
    "    inputFile2.write(\",\".join(row)+\"'\\n\")\n",
    "#     id+=1\n",
    "inputFile2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dishes', 'beets', 'cream', 'impression', 'iran', 'whos', 'talking', 'theres', 'many', 'youll', 'bettina', 'business', 'goliath', 'every', 'home', 'lettuce', 'hear', 'cargo', 'loud', 'teresa', 'bless', 'canon', 'tell', 'freeze', 'locked', 'door', 'asking', 'minute', 'come', 'beep', 'already', 'got', 'stands', 'expect', 'tired', 'opening', 'whole', 'anybody', 'questions', 'store', 'neighbors', 'turnip', 'place', 'yes', 'work', 'hatch', 'car', 'hoping', 'mind', 'death', 'mikey', 'way', 'want', 'question', 'close', 'sherlock', 'b4', 'voodoo', 'cut', 'neetu', 'witches', 'knock', 'could', 'cook', 'pleasure', 'tank', 'doesnt', 'please', 'hawaii', 'nun', 'joke', 'noise', 'coming', 'alex', 'tight', 'think', 'know', 'lady', 'identity', 'ailene', 'shave', 'pencil', 'yacht', 'figs', 'dark', 'doll', 'go', 'doorbell', 'annie', 'time', 'broken', 'adore', 'alexia', 'answered', 'everything', 'sound', 'song', 'see', 'soda', 'leena', 'instead', 'fit', 'use', 'santa', 'welcome', 'dozen', 'fb', 'open', 'going', 'neighborhood', 'owls', 'ice', 'icing', 'cereal', 'youve', 'rhino', 'lost', 'keyhole', 'sit', 'thou', 'zany', 'never', 'yeah', 'alexplain', 'ill', 'ben', 'aaron', 'youre', 'ailened', 'nice', 'dja', 'alfredo', 'cold', 'point', 'let', 'suitcase', 'dang', 'little', 'avenue', 'crisis', 'jess', 'volume', 'yodel', 'im', 'fine', 'alpaca', 'green', 'later', 'crazy', 'us', 'pack', 'old', 'seen', 'love', 'mustache', 'opened', 'body', 'idea', 'zoom', 'hoho', 'meet', 'fbi', 'trunk', 'looketh', 'say', 'abbott']\n"
     ]
    }
   ],
   "source": [
    "print(item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
